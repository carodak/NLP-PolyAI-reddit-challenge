{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kaggle_IVADO_2020.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN6NkOpV9QRO"
      },
      "source": [
        "Kaggle challenge IVADO: https://www.kaggle.com/c/semaine-ia-reddit-challenge\n",
        "I used TF-IDF representation for words and cleaned the dataset using scipy (tokenization, lematization, lower case and ponctuation). Got about 64% accuracy on the test set provided for the challenge. I tried to oversample my dataset with EDA method https://github.com/jasonwei20/eda_nlp but although it improved the f1_score on my CV (almost 90%), it does not generalize better with unknown data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZQ5srSS9XHB"
      },
      "source": [
        "Use GPU for faster training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1iM5LOC1bu2",
        "outputId": "627798f7-2135-40eb-d0b8-cf0384771148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stQSjucu9gZO"
      },
      "source": [
        "Load the driver helper and mount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1u4U04t9fyP",
        "outputId": "c1d921d0-e187-4889-a5f7-5709747840ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Load the driver helper and mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW-DYLtJ19Kl",
        "outputId": "6027671c-34e4-4c21-ebaf-fcd49b287fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "!ls '/content/drive/My Drive/ML/Kaggle_IVADO/'"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eda_train_sampling2.csv  preds2.csv  preds6.csv      train.csv\n",
            "eda_train_sampling3.csv  preds3.csv  preds.csv\n",
            "eda_train_sampling.csv\t preds4.csv  preds_last.csv\n",
            "Kaggle_IVADO_2020.ipynb  preds5.csv  test.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA5N7iOaFZOD"
      },
      "source": [
        "LOAD DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOvOsUSgE0Fm"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "def loadDataset():\n",
        "  #Load Packages\n",
        "  df_submission = pd.read_csv(\"/content/drive/My Drive/ML/Kaggle_IVADO/test.csv\")\n",
        "  df = pd.read_csv(\"/content/drive/My Drive/ML/Kaggle_IVADO/train.csv\")\n",
        "  #df = pd.read_csv(\"/content/drive/My Drive/ML/Kaggle_IVADO/eda_train_sampling3.csv\")\n",
        "  return df_submission, df"
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UJFaYzgVImV",
        "outputId": "ef28c5e7-6627-4010-e8e2-19b201e09767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "df_submission, df = loadDataset()\n",
        "#print(df)\n",
        "print(df_submission)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        id                                            comment\n",
            "0    14053  that s an interesting idea  but i think i d ha...\n",
            "1    14872  off to debtor s prison with you  meet your cel...\n",
            "2    26742  now all i am thinking is a normal sized facele...\n",
            "3    26927  if they do reproduce  or if more dragon eggs a...\n",
            "4    15099  gt  debunked this assertion with data showing ...\n",
            "..     ...                                                ...\n",
            "938  10489  forever grateful i graduated with minimal stud...\n",
            "939  27765  i wish i could tell you that thomas fought the...\n",
            "940  19911  if he is that good with mario  just imagine wh...\n",
            "941  10459  gt  castle made shoddy cheeses for almost 30 y...\n",
            "942  20096  you can emulate this incredibly easily if you ...\n",
            "\n",
            "[943 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgwk1B0LE1wS"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "def convert(data):\n",
        "  number = preprocessing.LabelEncoder()\n",
        "  data['comment'] = number.fit_transform(data['comment'])\n",
        "  data=data.fillna(-999) # fill holes with default value\n",
        "  return data\n"
      ],
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_AtxdqWVLk9",
        "outputId": "7add6bdc-0bc8-4149-ade5-51b0d511fb69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Load dataset\n",
        "dataset = np.array(df)\n",
        "dataset2 = np.array(df_submission)\n",
        "print(dataset.shape)\n",
        "X = dataset[:,0]\n",
        "Y = dataset[:,1]\n",
        "\n",
        "X_ = dataset2[:,1]\n",
        "#from sklearn.preprocessing import LabelEncoder\n",
        "#label_encoder = LabelEncoder()\n",
        "#Y = label_encoder.fit_transform(Y)\n",
        "print(X[0])\n",
        "print(X_[0])\n",
        "print(Y.shape)\n",
        "#print(sorted(Counter(Y).items()))"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3770, 2)\n",
            "would have been a million times more exciting if the living characters were nt shown  and jon opened his eyes at the end of the teaser \n",
            "that s an interesting idea  but i think i d have a really hard time taking it seriously and pretending to be nice to a virtual child when i knew that it could nt really understand me  especially in future sessions when i knew that the switch was going to happen \n",
            "(3770,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4u9LifCI_rp"
      },
      "source": [
        "TRAINING USEFUL METHODS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3oKTzIQJCOW",
        "outputId": "cf61e9f4-2808-4b7e-a3cc-93b93f28da1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!pip3 install pyriemann\n",
        "from pyriemann.utils.viz import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,precision_recall_fscore_support\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.model_selection import ShuffleSplit"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyriemann in /usr/local/lib/python3.6/dist-packages (0.2.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from pyriemann) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from pyriemann) (1.1.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->pyriemann) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->pyriemann) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utfLrQsjJgvq"
      },
      "source": [
        "#General confusion matrix that will be used for KNN and SVM\n",
        "def PlotPrectionConfLvl(preds,X,Y,model):\n",
        "  # make predictions on the testing data\n",
        "  print(\"[INFO] predicting confusion level ...\")\n",
        "  acc         = np.mean(preds == Y)\n",
        "  print(\"Classification accuracy: %f \" % (acc))\n",
        "\n",
        "  # plot the confusion matrices for both classifiers\n",
        "  names        = [\"Conf0\", \"Conf1\", \"Conf2\", \"Conf3\"]\n",
        "  #names        = possible_labels\n",
        "  plt.figure(0)\n",
        "  plot_confusion_matrix( Y, preds, names, title = str(type(model))+'\\nAccuracy:{0:.3f}'.format(acc))\n",
        "  plt.show()"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLe-5ir6JYFg"
      },
      "source": [
        "#Confusion matrix that will be used for LSTM\n",
        "def PlotPrectionConfLvlLSTM(preds,X,Y,model):\n",
        "  # make predictions on the testing data\n",
        "  print(\"[INFO] predicting confusion level ...\")\n",
        "  preds_       = preds.argmax(axis = -1)\n",
        "  acc         = np.mean(preds_ == Y.argmax(axis=-1))\n",
        "  print(\"Classification accuracy: %f \" % (acc))\n",
        "\n",
        "  # plot the confusion matrices for both classifiers\n",
        "  names        = [\"Conf0\", \"Conf1\", \"Conf2\", \"Conf3\"]\n",
        "  #names        = possible_labels\n",
        "  plt.figure(0)\n",
        "  plot_confusion_matrix(Y.argmax(axis = -1), preds_, names, title = str(type(model))+'\\nAccuracy:{0:.3f}'.format(acc))\n",
        "  plt.show()"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rujR-RD9JcbR"
      },
      "source": [
        "def Reshape2D(X):\n",
        "  print(np.array(list(X)).dtype)\n",
        "  print(X.shape)\n",
        "  X4 = X.reshape(X.shape[0],X.shape[1]*X.shape[2])\n",
        "  print(X4.shape)\n",
        "  #printList(X3)\n",
        "  return X4\n",
        "\n",
        "def Reshape3D(X):\n",
        "  print(np.array(list(X)).dtype)\n",
        "  print(X.shape)\n",
        "  X4 = X.reshape(X.shape[0],1,X.shape[1])\n",
        "  print(X4.shape)\n",
        "  #printList(X3)\n",
        "  return X4"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HpVr0JSJmFf"
      },
      "source": [
        "def PlotLearningCurveLSTM(lstm):\n",
        "  #print(lstm.history.keys())\n",
        "  # summarize history for accuracy\n",
        "  plt.plot(lstm.history['accuracy'])\n",
        "  plt.plot(lstm.history['val_accuracy'])\n",
        "  plt.plot(lstm.history['loss'])\n",
        "  plt.plot(lstm.history['val_loss'])\n",
        "  plt.title('model accuracy')\n",
        "  #plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.legend(['train accuracy', 'test accuracy','loss','validation loss'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1dg7yjeWcRa"
      },
      "source": [
        "USE SPICY TO PRE-PROCESS THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVqeFbmIWhNK",
        "outputId": "fb95ac2a-48cc-4a49-ce08-7ca3c9f77ea0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (50.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDCmZnSEWlpz"
      },
      "source": [
        "Tokenizing the Text: Separate each word from sentences, Cleaning Text Data: Removing Stopwords, Lexicon Normalization: a way of processing words that reduces them to their roots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8vOVOI1WmhQ"
      },
      "source": [
        "import string\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "\n",
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "# Create our list of stopwords\n",
        "nlp = spacy.load('en')\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "parser = English()\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = parser(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKsVE3lkWu49"
      },
      "source": [
        "Remove spaces and converts text into lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boPh1-FAWzdY"
      },
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "# Custom transformer using spaCy\n",
        "class predictors(TransformerMixin):\n",
        "    def transform(self, X, **transform_params):\n",
        "        # Cleaning Text\n",
        "        return [clean_text(text) for text in X]\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}\n",
        "\n",
        "# Basic function to clean the text\n",
        "def clean_text(text):\n",
        "    # Removing spaces and converting text into lowercase\n",
        "    return text.strip().lower()"
      ],
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5XizaHJW24t",
        "outputId": "05be7ea8-c28e-4256-d6cd-ac09f7e59ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "my_doc = X\n",
        "\n",
        "clean_txt = []\n",
        "for sentence in my_doc:\n",
        "  clean_txt.append(clean_text(sentence))\n",
        "print(clean_txt[0],clean_txt[1])"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "would have been a million times more exciting if the living characters were nt shown  and jon opened his eyes at the end of the teaser question for those in the know  i ve bought fallout 4 but have nt gotten to it yet i ve in fact made a point to finish fallout 3  and just beat the main quest last night  now i m onto the side missions and dlc  based on fallout 4  do people think this is going to be worth it  i ve seen reviews of the base game  and they seem mostly positive  but still note some issues with the game\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIUgYCd2W6en",
        "outputId": "90b5922e-8e3a-467a-832c-0b1e2db543f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Create list of word tokens\n",
        "token_list = []\n",
        "for sentence in clean_txt:\n",
        "  token_list.append(spacy_tokenizer(sentence))\n",
        "print(token_list[0],token_list[1])"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['million', 'times', 'exciting', 'living', 'characters', 'nt', 'shown', 'jon', 'opened', 'eyes', 'end', 'teaser'] ['question', 'know', 've', 'bought', 'fallout', '4', 'nt', 'gotten', 've', 'fact', 'point', 'finish', 'fallout', '3', 'beat', 'main', 'quest', 'night', 'm', 'missions', 'dlc', 'based', 'fallout', '4', 'people', 'think', 'going', 'worth', 've', 'seen', 'reviews', 'base', 'game', 'positive', 'note', 'issues', 'game']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whBDhB1rW_Ya"
      },
      "source": [
        "Vectorization Feature Engineering (TF-IDF)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLG6zkAMXAZP",
        "outputId": "e4befb58-a499-4fd8-d7a7-3212d8299259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X2 = np.array([', '.join(x) for x in token_list])\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "Y2 = encoder.fit_transform(df['subreddit'].values)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X2,Y2, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    TfidfTransformer(),\n",
        ")\n",
        "\n",
        "X_train = pipe.fit_transform(X_train.ravel()) \n",
        "X_test = pipe.transform(X_test)\n",
        "X_test2 = pipe.transform(X_)"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3016,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96O-n6ZUIw-B"
      },
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTjQkRLZLwL5"
      },
      "source": [
        "from sklearn.metrics import classification_report, f1_score, make_scorer\n",
        "\n",
        "def classification_report_with_f1_score(y_true, y_pred):\n",
        "    print (f1_score(y_true, y_pred, average='macro'))\n",
        "    print (classification_report(y_true, y_pred)) # print classification report\n",
        "    return f1_score(y_true, y_pred, average='macro') # return f1 score"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu-KV1aFK1eJ"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbscjVlLKwpO"
      },
      "source": [
        "#import the KNeighborsClassifier from sklearn\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "#import metrics model to check the accuracy\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_recall_fscore_support as score\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, GridSearchCV, TimeSeriesSplit\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "\n",
        "def KNNCV(X_trainn,Y_trainn,X_test,Y_test,k_range):\n",
        "  #Try running from k=1 through 25 and record testing accuracy\n",
        "  score = {}\n",
        "  scores_list = []\n",
        "  \n",
        "  #HYPERPARAMETER TUNING WITH CV\n",
        "  for k in k_range:\n",
        "\n",
        "\n",
        "   # kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state = 42)\n",
        "    \n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    #OVERSAMPLING\n",
        "    #imba_pipeline = make_pipeline(SMOTE(random_state=42), knn)\n",
        "        \n",
        "    print(\"Score for k=\",k)\n",
        "\n",
        "   # nested_score = cross_validate(knn, X_trainn, Y_trainn, cv=kfold, \\\n",
        "    #           scoring=make_scorer(classification_report_with_f1_score),return_train_score=True)\n",
        "\n",
        "    knn.fit(X_train, Y_train)\n",
        "    y_pred=knn.predict(X_test)\n",
        "    score = f1_score(Y_test,y_pred, average='macro')\n",
        "\n",
        "    scores_list.append(score)\n",
        "    print(\"y_pred: \",y_pred)\n",
        "\n",
        "    print(\"done for k=\",k)\n",
        "\n",
        "    print(score)\n",
        "    print(classification_report(Y_test,y_pred))\n",
        "    \n",
        "    #print(\"RESULTS: \")\n",
        "    #print(\"full res: \",nested_score)\n",
        "    #mean = nested_score[\"test_score\"].mean()\n",
        "    #print(\"mean CV for this k\", mean)\n",
        "    #scores_list.append(mean)\n",
        "    #print(\"mean CV for all k\", scores_list)\n",
        "    #print(\"STD for this k\", nested_score[\"test_score\"].std()*2)\n",
        "\n",
        "\n",
        "  return scores_list,knn,y_pred"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6b8nc3nL3ka",
        "outputId": "49d5749c-1519-47b8-8e16-3df326c9484b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "k_range = [23]\n",
        "scores_list,knn,y_pred = KNNCV(X_train,Y_train,X_test,Y_test,k_range)"
      ],
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score for k= 23\n",
            "y_pred:  [0 1 5 2 2 4 4 2 1 2 0 0 5 2 0 3 3 2 4 4 1 1 3 3 0 2 2 0 2 4 1 4 0 2 1 0 0\n",
            " 1 1 5 3 0 2 4 2 4 0 2 3 2 0 0 3 3 4 4 2 4 3 1 4 0 1 0 1 4 2 2 2 2 4 4 0 1\n",
            " 2 3 0 3 2 4 4 3 1 4 5 0 4 4 3 0 4 1 4 2 4 0 3 0 2 4 0 0 4 0 4 4 1 3 1 0 4\n",
            " 2 4 4 0 4 0 4 3 2 3 4 0 0 2 0 2 2 0 4 0 4 4 4 4 3 2 4 3 0 2 2 3 4 4 3 3 0\n",
            " 4 3 1 3 2 2 2 1 2 3 3 0 4 0 3 2 4 2 4 4 1 4 2 2 2 3 2 2 4 3 5 0 2 4 0 0 2\n",
            " 5 4 4 4 3 0 2 4 4 1 2 4 2 2 2 3 4 3 2 4 1 2 2 1 1 4 4 3 5 2 4 2 2 0 2 4 2\n",
            " 2 2 0 3 5 0 3 2 2 0 2 4 0 2 2 1 4 0 5 2 1 3 3 3 4 1 2 2 2 2 4 2 4 2 4 1 5\n",
            " 5 3 4 0 4 0 4 5 4 4 2 4 4 2 4 1 4 2 3 2 4 3 2 4 2 4 2 4 0 3 4 4 4 1 4 4 4\n",
            " 3 2 4 4 5 2 3 1 2 5 0 3 5 4 0 4 2 3 0 2 1 1 1 3 2 5 0 1 4 4 4 0 2 2 2 4 2\n",
            " 4 0 0 2 2 4 2 4 3 2 1 3 0 4 4 3 2 0 3 1 2 0 2 2 4 3 3 2 4 0 0 2 3 1 4 0 3\n",
            " 2 0 5 1 2 4 2 4 1 4 4 2 4 2 2 4 2 0 4 2 1 0 4 0 2 1 2 0 2 4 0 1 1 0 3 2 0\n",
            " 0 1 4 4 2 2 2 4 2 5 4 3 1 5 2 5 0 0 4 5 4 2 4 1 4 3 4 4 4 2 1 0 1 2 2 2 4\n",
            " 0 2 2 0 0 0 4 4 3 5 4 3 1 2 2 1 0 2 4 0 1 4 4 1 2 2 0 2 2 4 2 2 1 0 2 4 4\n",
            " 4 3 2 2 3 2 0 1 0 4 4 4 4 3 0 2 0 4 2 4 4 1 4 2 4 4 4 0 3 0 3 2 5 4 4 4 2\n",
            " 0 4 2 4 4 4 4 0 3 4 1 4 0 4 4 4 4 4 4 2 0 5 1 1 4 4 1 0 4 3 2 4 2 5 2 2 1\n",
            " 4 3 4 4 2 4 4 2 5 2 2 4 2 5 0 2 4 4 2 0 3 2 4 3 4 4 1 2 1 2 3 0 3 4 3 2 2\n",
            " 4 0 4 4 3 0 4 3 1 4 0 1 4 3 0 3 4 0 4 0 1 4 4 2 2 3 2 1 2 2 3 3 4 2 4 4 1\n",
            " 2 1 4 0 2 2 0 4 1 0 4 0 3 3 3 1 4 2 2 1 4 0 2 2 3 2 2 4 1 2 2 4 4 2 3 2 3\n",
            " 0 2 0 2 0 0 1 1 4 1 2 0 4 0 0 2 0 0 2 5 4 1 2 3 3 1 5 3 1 2 4 4 2 1 2 4 0\n",
            " 2 2 2 2 1 3 1 1 0 0 2 3 4 3 2 1 4 3 4 0 4 3 4 4 4 1 1 4 4 4 0 5 0 4 1 3 0\n",
            " 2 2 2 3 3 3 2 0 3 3 4 2 2 2]\n",
            "done for k= 23\n",
            "0.5973457979351184\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.50      0.51       127\n",
            "           1       0.66      0.62      0.64        92\n",
            "           2       0.63      0.75      0.68       166\n",
            "           3       0.58      0.47      0.52       124\n",
            "           4       0.62      0.76      0.69       176\n",
            "           5       0.90      0.39      0.55        69\n",
            "\n",
            "    accuracy                           0.62       754\n",
            "   macro avg       0.65      0.58      0.60       754\n",
            "weighted avg       0.63      0.62      0.61       754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c7FLohWmVIx",
        "outputId": "a6201615-2f89-4e09-c807-2614c56d9287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(y_pred.shape)\n",
        "preds = encoder.inverse_transform(y_pred)\n",
        "print(preds)\n",
        "np.savetxt(\"/content/drive/My Drive/ML/Kaggle_IVADO/preds.csv\", preds, delimiter=\",\",fmt='%s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7540,)\n",
            "['news' 'gameofthrones' 'gaming' ... 'gameofthrones' 'science' 'news']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnv_qRQPpE0O"
      },
      "source": [
        "\n",
        "preds = knn.predict(X_test2)\n",
        "preds = encoder.inverse_transform(preds)\n",
        "print(preds)\n",
        "np.savetxt(\"/content/drive/My Drive/ML/Kaggle_IVADO/preds.csv\", preds, delimiter=\",\",fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTYzYB_aZfZu"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU-20kWmf_ht"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.models import Model\n",
        "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "%matplotlib inline"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyNJdZ3Hn5gA"
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def recall_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu6zevbPVjTr",
        "outputId": "70ffc0ee-e2d1-4d9a-b7c7-aea3704dd3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "import re\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "tokenizer.fit_on_texts(token_list)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "X2 = tokenizer.texts_to_sequences(token_list)\n",
        "X2 = pad_sequences(X2, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X2.shape)\n",
        "\n",
        "X2_ = tokenizer.texts_to_sequences(df_submission['comment'].values)\n",
        "X2_ = pad_sequences(X2_, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "\n",
        "Y2 = encoder.fit_transform(df['subreddit'].values)\n",
        "\n",
        "print(X2.shape)\n",
        "print(X2_.shape)\n",
        "print(Y2.shape)\n",
        "\n",
        "#gameofthrones = 1\n",
        "#gaming = 2\n",
        "# funny = 0\n",
        "# news = 3\n",
        "# politics = 4\n",
        "# science = 5"
      ],
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 13827 unique tokens.\n",
            "Shape of data tensor: (3770, 50)\n",
            "(3770, 50)\n",
            "(943, 50)\n",
            "(3770,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgtLqxZLh7__",
        "outputId": "6b2a2d5b-c020-4e61-a5ae-c5c1435a35d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X2,Y2, test_size = 0.3, random_state = 42)\n",
        "#X_train, Y_train = SmoteSampling(X_train, Y_train)\n",
        "#X_train,Y_train = NaiveSampling(X_train,Y_train)\n",
        "\n",
        "print(Y_train)\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "\n",
        "Y_train = np.array(Y_train)\n",
        "Y_test = np.array(Y_test)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(Y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(Y_test.shape)"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 2 0 ... 0 0 0]\n",
            "(2639, 50)\n",
            "(2639, 6)\n",
            "(1131, 50)\n",
            "(1131, 6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvSvalmQZgWn"
      },
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, SpatialDropout1D, LSTM, Embedding, TimeDistributed, Dropout, GaussianNoise, Activation\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "def LSTMModel(X_train, X_valid, Y_train, Y_valid):\n",
        "  nb_classes = Y_train.shape[1]\n",
        "  epochs = 15\n",
        "  batch_size = 64\n",
        "  hidden_size = 200\n",
        "\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[1]))\n",
        "  model.add(SpatialDropout1D(0.2))\n",
        "  model.add(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2))\n",
        "  model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])\n",
        "\n",
        "  history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid, Y_valid), verbose=1, callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
        "  \n",
        "  model2 = model\n",
        "  y_pred = model2.predict(X_valid, batch_size=batch_size, verbose=1)\n",
        "  y_pred_=np.argmax(y_pred, axis=1)\n",
        "  Y_valid_ = np.argmax(Y_valid, axis=1)\n",
        "  print(classification_report(Y_valid_,y_pred_))\n",
        "\n",
        "  return model,X_train, X_valid, Y_train, Y_valid,y_pred,history"
      ],
      "execution_count": 179,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsG3ubd6iHxO",
        "outputId": "b19fa7f4-3f35-416c-8977-84a32940ae84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "lstm,Xtrain,Xvalid,Ytrain,Yvalid,ypred,history = LSTMModel(X_train, X_test, Y_train, Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "42/42 [==============================] - 17s 405ms/step - loss: 1.7399 - acc: 0.2330 - f1_m: 0.0000e+00 - precision_m: 0.0000e+00 - recall_m: 0.0000e+00 - val_loss: 1.7055 - val_acc: 0.2520 - val_f1_m: 0.0000e+00 - val_precision_m: 0.0000e+00 - val_recall_m: 0.0000e+00\n",
            "Epoch 2/15\n",
            "42/42 [==============================] - 17s 402ms/step - loss: 1.5703 - acc: 0.3717 - f1_m: 0.0341 - precision_m: 0.2704 - recall_m: 0.0205 - val_loss: 1.5280 - val_acc: 0.3837 - val_f1_m: 0.0512 - val_precision_m: 0.6463 - val_recall_m: 0.0269\n",
            "Epoch 3/15\n",
            "42/42 [==============================] - 17s 400ms/step - loss: 1.1072 - acc: 0.5911 - f1_m: 0.3371 - precision_m: 0.8060 - recall_m: 0.2284 - val_loss: 1.2307 - val_acc: 0.5270 - val_f1_m: 0.4226 - val_precision_m: 0.7621 - val_recall_m: 0.2942\n",
            "Epoch 4/15\n",
            "42/42 [==============================] - 17s 397ms/step - loss: 0.6205 - acc: 0.7870 - f1_m: 0.7428 - precision_m: 0.8778 - recall_m: 0.6469 - val_loss: 1.1684 - val_acc: 0.5747 - val_f1_m: 0.5432 - val_precision_m: 0.7298 - val_recall_m: 0.4343\n",
            "Epoch 5/15\n",
            "42/42 [==============================] - 17s 398ms/step - loss: 0.3065 - acc: 0.9163 - f1_m: 0.9086 - precision_m: 0.9476 - recall_m: 0.8735 - val_loss: 1.2620 - val_acc: 0.5977 - val_f1_m: 0.5934 - val_precision_m: 0.6621 - val_recall_m: 0.5384\n",
            "Epoch 6/15\n",
            "42/42 [==============================] - 17s 398ms/step - loss: 0.1689 - acc: 0.9617 - f1_m: 0.9561 - precision_m: 0.9770 - recall_m: 0.9366 - val_loss: 1.1685 - val_acc: 0.6012 - val_f1_m: 0.5996 - val_precision_m: 0.7001 - val_recall_m: 0.5258\n",
            "Epoch 7/15\n",
            "42/42 [==============================] - 16s 391ms/step - loss: 0.0779 - acc: 0.9871 - f1_m: 0.9835 - precision_m: 0.9916 - recall_m: 0.9757 - val_loss: 1.3571 - val_acc: 0.6172 - val_f1_m: 0.6172 - val_precision_m: 0.6823 - val_recall_m: 0.5644\n",
            "18/18 [==============================] - 1s 46ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.52      0.47      0.49       199\n",
            "           1       0.74      0.72      0.73       137\n",
            "           2       0.67      0.73      0.70       239\n",
            "           3       0.45      0.54      0.49       190\n",
            "           4       0.71      0.70      0.70       252\n",
            "           5       0.66      0.46      0.55       114\n",
            "\n",
            "    accuracy                           0.62      1131\n",
            "   macro avg       0.62      0.60      0.61      1131\n",
            "weighted avg       0.62      0.62      0.62      1131\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p6xyyp2gONp"
      },
      "source": [
        "preds = lstm.predict(X2_)\n",
        "preds = np.argmax(preds, axis=1)\n",
        "#gameofthrones = 1\n",
        "#gaming = 2\n",
        "# funny = 0\n",
        "# news = 3\n",
        "# politics = 4\n",
        "# science = 5\n",
        "predictions = []\n",
        "for pred in preds:\n",
        "  if pred == 0:\n",
        "    predictions = np.append(predictions,'funny')\n",
        "  elif pred == 1:\n",
        "    predictions = np.append(predictions,'gameofthrones')\n",
        "  elif pred == 2:\n",
        "    predictions = np.append(predictions,'gaming')\n",
        "  elif pred == 3:\n",
        "    predictions = np.append(predictions,'news')\n",
        "  elif pred == 4:\n",
        "    predictions = np.append(predictions,'politics')\n",
        "  elif pred == 5:\n",
        "    predictions = np.append(predictions,'science')\n",
        "  else:\n",
        "    print(\"error\")\n",
        "print(predictions)\n",
        "np.savetxt(\"/content/drive/My Drive/ML/Kaggle_IVADO/preds2.csv\", predictions, delimiter=\",\",fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gn9r-QXZTx"
      },
      "source": [
        "SVM MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR2HvFsAYJ25"
      },
      "source": [
        "Vectorization TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeXK3qCaYJRt",
        "outputId": "c7bfcfb7-e30a-4e75-816b-a1f66e6997b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X2 = np.array([', '.join(x) for x in token_list])\n",
        "\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "Y2 = encoder.fit_transform(df['subreddit'].values)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X2,Y2, test_size = 0.2, random_state = 42)\n",
        "\n",
        "print(X_train.shape)\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(\n",
        "    CountVectorizer(),\n",
        "    TfidfTransformer(),\n",
        ")\n",
        "\n",
        "X_train = pipe.fit_transform(X_train.ravel()) \n",
        "X_test = pipe.transform(X_test)\n",
        "X_test2 = pipe.transform(X_)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3016,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehl2JaZmDYRD"
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "def SVM(X_train, X_test, Y_train, Y_test):\n",
        "  classifier = SVC(C=2.01, kernel='linear', degree=3, gamma='auto')\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  cr = classification_report(Y_test,y_pred)\n",
        "  score = f1_score(Y_test,y_pred, average='macro')\n",
        "  print(score)\n",
        "  print(cr)\n",
        "  return classifier,y_pred"
      ],
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uCN3nMSDn0K",
        "outputId": "ed2683be-7f49-462a-d1d0-c4dc99575c61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "svm, ypred = SVM(X_train, X_test, Y_train, Y_test)"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.669200997287071\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.58      0.56       127\n",
            "           1       0.83      0.71      0.76        92\n",
            "           2       0.64      0.74      0.69       166\n",
            "           3       0.55      0.65      0.60       124\n",
            "           4       0.79      0.68      0.73       176\n",
            "           5       0.78      0.58      0.67        69\n",
            "\n",
            "    accuracy                           0.67       754\n",
            "   macro avg       0.69      0.66      0.67       754\n",
            "weighted avg       0.68      0.67      0.67       754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buSO43a5H3YL"
      },
      "source": [
        "preds = svm.predict(X_test2)\n",
        "preds = encoder.inverse_transform(preds)\n",
        "#print(preds)\n",
        "np.savetxt(\"/content/drive/My Drive/ML/Kaggle_IVADO/preds_last.csv\", preds, delimiter=\",\",fmt='%s')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv9EVyN7iyYO"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "def LR(X_train, X_test, Y_train, Y_test):\n",
        "  classifier = LogisticRegression()\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  cr = classification_report(Y_test,y_pred)\n",
        "  score = f1_score(Y_test,y_pred, average='macro')\n",
        "  print(score)\n",
        "  print(cr)\n",
        "  return classifier,y_pred"
      ],
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDmx1W64yjjN",
        "outputId": "93437cf0-2907-498e-b86c-d776de893563",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "logreg, ypred = LR(X_train, X_test, Y_train, Y_test)"
      ],
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6559052815752969\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.48      0.54       127\n",
            "           1       0.91      0.64      0.75        92\n",
            "           2       0.66      0.81      0.73       166\n",
            "           3       0.51      0.73      0.60       124\n",
            "           4       0.75      0.76      0.75       176\n",
            "           5       0.93      0.41      0.57        69\n",
            "\n",
            "    accuracy                           0.67       754\n",
            "   macro avg       0.73      0.64      0.66       754\n",
            "weighted avg       0.70      0.67      0.67       754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-cTMoqZyUZz"
      },
      "source": [
        "preds = logreg.predict(X_test2)\n",
        "preds = encoder.inverse_transform(preds)\n",
        "#print(preds)\n",
        "np.savetxt(\"/content/drive/My Drive/ML/Kaggle_IVADO/preds.csv\", preds, delimiter=\",\",fmt='%s')"
      ],
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABQIhaUGM1OM"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def RF(X_train, X_test, Y_train, Y_test):\n",
        "  classifier = RandomForestClassifier(n_estimators=1000,max_depth=100, random_state=0)\n",
        "  classifier.fit(X_train, Y_train)\n",
        "  y_pred = classifier.predict(X_test)\n",
        "  cr = classification_report(Y_test,y_pred)\n",
        "  score = f1_score(Y_test,y_pred, average='macro')\n",
        "  print(score)\n",
        "  print(cr)\n",
        "  return classifier,y_pred"
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1B47ZuZNwXd",
        "outputId": "486fb0ad-a4aa-483e-daf1-f70b5b9a00eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "rf, ypred = RF(X_train, X_test, Y_train, Y_test)"
      ],
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.634535029475987\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.41      0.68      0.51       127\n",
            "           1       0.90      0.66      0.76        92\n",
            "           2       0.70      0.74      0.72       166\n",
            "           3       0.52      0.60      0.56       124\n",
            "           4       0.87      0.64      0.73       176\n",
            "           5       0.84      0.38      0.52        69\n",
            "\n",
            "    accuracy                           0.64       754\n",
            "   macro avg       0.71      0.62      0.63       754\n",
            "weighted avg       0.70      0.64      0.65       754\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}